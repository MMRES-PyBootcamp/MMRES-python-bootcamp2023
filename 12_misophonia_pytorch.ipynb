{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMRES-PyBootcamp/MMRES-python-bootcamp2022/blob/main/12_misophonia_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cREsULcqQdIi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-S4nOfkQdIm"
      },
      "outputs": [],
      "source": [
        "# Load package with its corresponding alias\n",
        "import pandas as pd\n",
        "\n",
        "# Reading an Excel SpreadSheet and storing it in as a DataFrame called `df`\n",
        "df = pd.read_excel('https://github.com/MMRES-PyBootcamp/MMRES-python-bootcamp2022/blob/main/datasets/misophoinia_data.xlsx?raw=true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-s9R6sEQdIn"
      },
      "source": [
        "Before we can get started, we need to manipulate the data a little bit to make it conform to be able to be processed in pytorch.  \n",
        "This includes converting some `string` lists into numerical values, e.g. roman letters to numerical values, `\"III\" -> 3`, or turning yes/no strings to categorical vectors `\"si\" -> [1, 0]` (and `\"no\" -> [0, 1]`).  \n",
        "Further, we need to transform the data into torch tensors.  \n",
        "Note that there are a large number of different ways of doing this, some more and some less efficient. A rule of thumb in python is to avoid for loops and try to use functions like `map` that are parallelized as much as possible. Here it is not _very_ important since the dataset is very small, but it is something to keep in mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHR9_g1EQdIo"
      },
      "outputs": [],
      "source": [
        "# Transform roman numbers to float numbers\n",
        "def roman_to_numeric(roman):\n",
        "    if roman == \"I\":\n",
        "        return 1.\n",
        "    elif roman ==\"II\":\n",
        "        return 2.\n",
        "    elif roman ==\"III\":\n",
        "        return 3.\n",
        "    return roman\n",
        "CLASE = df[\"CLASE\"].values\n",
        "CLASE = np.array(list(map(roman_to_numeric, CLASE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDMQFQmoQdIp"
      },
      "outputs": [],
      "source": [
        "tensors = [\n",
        "    df[\"Estado.dic\"].values,\n",
        "    df[\"ansiedad.rasgo\"].values,\n",
        "    df[\"ansiedad.estado\"].values,\n",
        "    df[\"ansiedad.medicada.dic\"].values,\n",
        "    df[\"depresion\"].values,\n",
        "    df[\"Edad\"].values,\n",
        "    CLASE,\n",
        "    df[\"Angulo_convexidad\"].values,\n",
        "    df[\"Angulo_cuelloYtercio\"].values,\n",
        "    df[\"Subnasal_H\"].values\n",
        "]\n",
        "\n",
        "mask = ~np.isnan(tensors[0])\n",
        "\n",
        "for i, t in enumerate(tensors):\n",
        "    tensors[i] = torch.tensor(t[mask], dtype=torch.float)\n",
        "\n",
        "data = torch.stack(tensors).transpose(1,0)\n",
        "#data = data[:, None] # this is because the second dimension is reserved for channels, which we dont have here\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws1UY-3BQdIq"
      },
      "source": [
        "We now have 123 datapoints of 10 features at our disposal. We want to map each data point to a probability distribution `(p(misofonia), p(no_misofonia))` consisting of two values. As labels, we use the `\"Misofonia\"` variables that are given in as `\"si\"` and `\"no\"` strings. So we need to transform those into so-called categorical vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4g5LRHeQdIr"
      },
      "outputs": [],
      "source": [
        "# Generate the target labels and transform \"si\"/\"no\" to [1,0] and [0,1], corresponding to the probabilities [p(misofonia), p(no_misofonia)]\n",
        "def sino_to_categorical(string, num_classes=2):\n",
        "    if string==\"si\":\n",
        "        y = 1\n",
        "    elif string==\"no\":\n",
        "        y = 0\n",
        "    return y #torch.eye(num_classes, dtype=torch.float)[y]\n",
        "\n",
        "labels = df[\"Misofonia\"].values\n",
        "labels = labels[mask] # remove nans\n",
        "labels = torch.tensor(list(map(sino_to_categorical, labels)))\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVEAQEPDQdIs"
      },
      "source": [
        "We now have all the ingredients, our `data` and categorical vector `labels`, to train our model.  \n",
        "PyTorch works best when using their efficient data handlers, for that we need some further processing.  \n",
        "\n",
        "We also dont want to use all our data for training, but leave some data for testing of the model on data it has not yet seen (during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEatAGVBQdIs"
      },
      "outputs": [],
      "source": [
        "dataset = torch.utils.data.TensorDataset(data, labels) # some torch specific transformations of the data into a TensorDataset\n",
        "\n",
        "# Split all data into a train and test set\n",
        "N_data = len(data)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [N_data - 20, 20])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=12)  # for efficient processing, you will want to create a DataLoader object\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=12)\n",
        "\n",
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOypzqvCQdIt"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(10, 512), # need to set input dimension to size of data (number of features = 10)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 2) # need to set output to dimension of number of categories (2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        out = torch.sigmoid(logits)\n",
        "        return out\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxDIC5uQQdIu"
      },
      "outputs": [],
      "source": [
        "# check if dimensions match by computing an output\n",
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    break\n",
        "\n",
        "print(X.shape, y.shape, pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWSd1H-RQdIu"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whSGZJeVQdIv"
      },
      "outputs": [],
      "source": [
        "loss_fn(pred, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34hr05KDQdIv"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 2 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # compute accuracy\n",
        "    train_acc = correct/size * 100\n",
        "    return loss, train_acc\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    test_acc = 100*correct\n",
        "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4CeCWTgQdIw"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train_losss, train_accs = [], []\n",
        "test_losss, test_accs = [], []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\")\n",
        "    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loss, test_acc = test(test_dataloader, model, loss_fn)\n",
        "    train_losss.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    test_losss.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQIJ7XDEQdIw"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(figsize=(10,5), ncols=2)\n",
        "\n",
        "ax = axs[0]\n",
        "ax.plot(train_losss, label=\"train\")\n",
        "ax.plot(test_losss, \"x-\", label=\"test\")\n",
        "\n",
        "ax.set_title(\"loss\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.legend()\n",
        "\n",
        "ax = axs[1]\n",
        "ax.plot(train_accs, label=\"train\")\n",
        "ax.plot(test_accs, \"x-\", label=\"test\")\n",
        "\n",
        "ax.set_title(\"accuracy\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8lQ-glAQdIw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pennylane')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9686d7135c507acf535d05158d41b23f151b7f97a7a59cb8e05a88553ea2b8de"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}